{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.198914Z",
     "end_time": "2023-04-25T15:40:15.213768Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora,models\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "swearwords = \"\"\"cumbubble\n",
    "fuck\n",
    "fuck you\n",
    "shitbag\n",
    "shit\n",
    "piss off\n",
    "asshole\n",
    "dickweed\n",
    "cunt\n",
    "son of a bitch\n",
    "fuck trumpet\n",
    "bastard\n",
    "bitch\n",
    "damn\n",
    "bollocks\n",
    "bugger\n",
    "cocknose\n",
    "bloody hell\n",
    "knobhead\n",
    "choad\n",
    "bitchtits\n",
    "crikey\n",
    "rubbish\n",
    "pissflaps\n",
    "shag\n",
    "wanker\n",
    "talking the piss\n",
    "twat\n",
    "arsebadger\n",
    "jizzcock\n",
    "cumdumpster\n",
    "shitmagnet\n",
    "scrote\n",
    "twatwaffle\n",
    "thundercunt\n",
    "dickhead\n",
    "shitpouch\n",
    "jizzstain\n",
    "nonce\n",
    "pisskidney\n",
    "wazzock\n",
    "cumwipe\n",
    "fanny\n",
    "bellend\n",
    "pisswizard\n",
    "knobjockey\n",
    "cuntpuddle\n",
    "dickweasel\n",
    "quim\n",
    "bawbag\n",
    "fuckwit\n",
    "tosspot\n",
    "cockwomble\n",
    "twat face\n",
    "cack\n",
    "flange\n",
    "clunge\n",
    "dickfucker\n",
    "fannyflaps\n",
    "wankface\n",
    "shithouse\n",
    "gobshite\n",
    "jizzbreath\n",
    "todger\n",
    "nutsack\n",
    "motherfucker\n",
    "fuck\n",
    "fucking\n",
    "dick\n",
    "pussy\n",
    "ha\n",
    "word\n",
    "yeah\n",
    "cause\n",
    "somebody\n",
    "someone\n",
    "gotta\n",
    "everyone\n",
    "question\n",
    "sex\n",
    "everybody\n",
    "question\n",
    "bit\n",
    "idea\n",
    "\"\"\".split('\\n')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "stopwords.update(set(swearwords))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('comedy_cleaned.csv', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.203830Z",
     "end_time": "2023-04-25T15:40:15.351624Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['year'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.352664Z",
     "end_time": "2023-04-25T15:40:15.358045Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import re\n",
    "#\n",
    "# def remove_non_printable_chars(text):\n",
    "#     return re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "#\n",
    "# df['text'] = df['text'].apply(remove_non_printable_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.357981Z",
     "end_time": "2023-04-25T15:40:15.359748Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "docs = df['text'].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.360683Z",
     "end_time": "2023-04-25T15:40:15.362127Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a dictionary\n",
    "years = {\n",
    "    'Pre-2000': [1900, 2000],\n",
    "    '2000-2010': [2000, 2010],\n",
    "    '2010-2020': [2010, 2020],\n",
    "    'Post-2020': [2020, 2100]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.362974Z",
     "end_time": "2023-04-25T15:40:15.364392Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find rows in df where year is between 2000 and 2010\n",
    "for (key, value) in years.items():\n",
    "    df1 = df.loc[(df['year'] >= value[0]) & (df['year'] < value[1])]\n",
    "    years[key].append(df1['text'].tolist())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.366378Z",
     "end_time": "2023-04-25T15:40:15.367830Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for (key, value) in years.items():\n",
    "    print(key, len(value[2]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.368840Z",
     "end_time": "2023-04-25T15:40:15.372641Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def extract_nouns(tokenized_sentences):\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "\n",
    "    nouns  = [[x for x, tag in pos_tag(sentence) if tag in noun_tags] for sentence in tokenized_sentences]\n",
    "\n",
    "    return nouns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:40:15.378207Z",
     "end_time": "2023-04-25T15:40:15.480029Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topics = []\n",
    "for (key, value) in years.items():\n",
    "    docs = value[2]\n",
    "    for idx in range(len(docs)):\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "        docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "    docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    docs = [[token for token in doc if token not in stopwords] for doc in docs]\n",
    "    docs = [[token for token in doc if len(token) > 1] for doc in docs]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n",
    "\n",
    "    docs = extract_nouns(docs)\n",
    "\n",
    "    bigram = Phrases(docs,min_count = 10)\n",
    "    for idx in range(len(docs)):\n",
    "        for token in bigram[docs[idx]]:\n",
    "            if '_' in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                docs[idx].append(token)\n",
    "    dictionary = Dictionary(docs)\n",
    "    dictionary.filter_extremes(no_below=10, no_above=0.8)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    word_counts = [[(dictionary[id], count) for id, count in line] for line in corpus]\n",
    "    sort_token = sorted(dictionary.items(),key=lambda k:k[0], reverse = False)\n",
    "    unique_token = [token for (ID,token) in sort_token]\n",
    "    matrix = gensim.matutils.corpus2dense(corpus,num_terms=len(dictionary),dtype = 'int')\n",
    "\n",
    "    matrix = matrix.T #transpose the matrix\n",
    "    matrix_df = pd.DataFrame(matrix, columns=unique_token)\n",
    "\n",
    "    num_topics = 1\n",
    "    chunksize = 2000\n",
    "    #chenksize is the number of documents to be used in each training chunk.\n",
    "    passes = 20 #The number of model training in the whole corpus\n",
    "    iterations = 100 #Number of iterations of per document\n",
    "    eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "    temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "    id2word = dictionary.id2token\n",
    "\n",
    "    lda = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        alpha='auto',\n",
    "        eta='auto',\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "    )\n",
    "    print(lda)\n",
    "    # for i,topic in lda.print_topics(10):\n",
    "    #     print(f'Top 10 words for topic #{i+1}:')\n",
    "    #     print(topic)\n",
    "    #     print('\\n')\n",
    "\n",
    "    def get_top_topics(lda_model, num_topics=10, num_words=10):\n",
    "        top_topics = lda_model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "        topics = []\n",
    "\n",
    "        for topic in top_topics:\n",
    "            topic_words = [word for word, _ in topic[1]]\n",
    "            topics.append(topic_words)\n",
    "\n",
    "        return topics\n",
    "\n",
    "    top_topics = get_top_topics(lda, num_topics=1, num_words=10)\n",
    "    topics.append(top_topics[0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T15:39:23.534118Z",
     "end_time": "2023-04-25T15:39:24.873121Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lda.show_topics()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(topics).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
